
@article{Bao2021,
  title = {Utilization of {{Semantic Planes}}: {{Improved Localization}} and {{Dense Semantic Map}} for {{Monocular SLAM}} in {{Urban Environment}}},
  shorttitle = {Utilization of {{Semantic Planes}}},
  author = {Bao, Yaoqi and Pan, Yun and Yang, Zhe and Huan, Ruohong},
  year = {2021},
  month = jul,
  journal = {IEEE Robotics and Automation Letters},
  volume = {6},
  number = {3},
  pages = {6108--6115},
  issn = {2377-3766},
  abstract = {In this letter, we propose a novel semantic direct monocular simultaneous localization and mapping (SLAM) system that fuses the semantic information obtained by an advanced deep neural network (DNN) into direct sparse odometry with loop closure(LDSO), with the purpose of improving the localization accuracy and building a dense semantic map of the urban environment. For localization, we apply a point reselection strategy based on coarse semantic plane (CSP) constraints to discard static points inconsistent with the nearby co-plane points of the same semantic class and dynamic points beyond the visible range. Moreover, a point group movement consistency (PGMC) check is utilized to decrease the impact of moving dynamic objects. For the dense semantic map, we model numerous small semantic planes from well-estimated points to measure the depth of each static pixel, rather than conduct stereo matching. Experimental results show that our method is more accurate than LDSO and comparable with ORB-SLAM in terms of localization. Moreover, it is capable of building a dense semantic map of the urban environment for better scene understanding.},
  copyright = {All rights reserved},
  keywords = {autonomous vehicle navigation,Buildings,Cameras,Geometry,Location awareness,Semantic scene understanding,Semantics,Simultaneous localization and mapping,SLAM,Three-dimensional displays},
  annotation = {ZSCC: 0000000},
  file = {/Users/zyz/Zotero/storage/RVCGSIC7/Bao et al. - 2021 - Utilization of Semantic Planes Improved Localizat.pdf;/Users/zyz/Zotero/storage/CKGSCM85/authors.html}
}

@article{Bao2022,
  title = {Semantic-{{Direct Visual Odometry}}},
  author = {Bao, Yaoqi and Yang, Zhe and Pan, Yun and Huan, Ruohong},
  year = {2022},
  month = jul,
  journal = {IEEE Robotics and Automation Letters},
  volume = {7},
  number = {3},
  pages = {6718--6725},
  issn = {2377-3766},
  abstract = {Traditional direct SLAM methods formulate the camera pose and map estimation as minimization of the photometric error, which is tackled by the Gauss-Newton algorithm or the Levenberg-Marquardt algorithm in the optimization. However, the convexity of the photometric error only holds in a small region due to the characteristics of the convexity for grayscale images. Thus, the system may be stuck in sub-optimal local minima when tracking points have large displacement. Unlike grayscale images, the semantic probabilities omit the details inside the semantic objects while mainly reforming on the boundary of semantic objects, which has better convexity for large displacement. In this letter, we propose a novel semantic-direct visual odometry (SDVO), exploiting the direct alignment of semantic probabilities. By constructing the joint error function based on grayscale images and semantic probabilities, the joint error function achieves better convexity contrary to the photometric error. Consequently, the proposed system moves towards optima with steady steps in the optimization iterations. Experimental results on the challenging real-world dataset demonstrate a significant improvement over the baseline by integrating the direct alignment of semantic probabilities.},
  copyright = {All rights reserved},
  keywords = {Autonomous vehicle navigation,Gray-scale,Roads,semantic scene understanding,Semantics,Simultaneous localization and mapping,SLAM,Three-dimensional displays,Visual odometry,Visualization},
  file = {/Users/zyz/Zotero/storage/ZW2FEJSA/Bao et al. - 2022 - Semantic-Direct Visual Odometry.pdf;/Users/zyz/Zotero/storage/96ABQKHG/9779967.html}
}

@inproceedings{Yang2017,
  title = {Hybrid Orientation Filter Aided Indoor Tracking for Pedestrians Using a Smartphone},
  booktitle = {2017 13th {{IEEE International Conference}} on {{Control Automation}} ({{ICCA}})},
  author = {Yang, Z. and Pan, Y. and Zhang, L.},
  year = {2017},
  month = jul,
  pages = {107--112},
  abstract = {In this paper, we proposed a novel approach called Hybrid Orientation Filter which deploys a combination of the information filter and the complementary filter to achieve accurate heading estimation. Base on this approach, a Hybrid Orientation Filter aided pedestrian dead reckoning system with utilization of the visual gyroscope is proposed and implemented on a smartphone, offering real-time indoor tracking for pedestrians without map information or infrastructure. Experiment results show that the proposed dead reckoning system achieves typically sub-meter accuracy in position over 207 meters, and an average error of 2.55 degrees in heading over 1800 degrees.},
  copyright = {All rights reserved},
  keywords = {complementary filter,gyroscopes,Gyroscopes,heading estimation,hybrid orientation filter aided indoor tracking,information filter,information filters,Information filters,Magnetic separation,Optical filters,pedestrian dead reckoning system,pedestrians,radio tracking,real-time indoor tracking,Sensors,smart phones,smartphone,visual gyroscope,visualization,Visualization},
  annotation = {ZSCC: 0000003},
  file = {/Users/zyz/Zotero/storage/PXYLH5S9/Yang et al. - 2017 - Hybrid orientation filter aided indoor tracking fo.pdf}
}

@article{Yang2019,
  title = {Real-{{Time Infrastructureless Indoor Tracking}} for {{Pedestrian Using}} a {{Smartphone}}},
  author = {Yang, Zhe and Pan, Yun and Tian, Qinglin and Huan, Ruohong},
  year = {2019},
  month = nov,
  journal = {IEEE Sensors Journal},
  volume = {19},
  number = {22},
  pages = {10782--10795},
  issn = {2379-9153},
  abstract = {Demands for accurate pedestrian indoor tracking on mobile platforms have been increasing rapidly these years, however, conventional tracking system based on dead reckoning suffers from inherent sensor drift, which leads to bad performance in long-term tracking. In this paper, an infrastructureless pedestrian dead reckoning system called iPDR is proposed and implemented on a ready-to-use smartphone, offering real-time indoor tracking for pedestrian without previous knowledge of the target area. The iPDR fuses the sensor data using a novel filter approach coined as hybrid orientation filter, which deploys a combination of the information filter and the complementary filter to achieve accurate heading estimation. A fast loop closure detection method called rapid loop detection is also presented in the iPDR to calibrate the tracking trajectory significantly whenever encountering a loop. Experiment results show that the iPDR system achieves typically sub-meter accuracy in localization and has an average orientation error of 2.89 degrees over a total turning of 1800 degrees.},
  copyright = {All rights reserved},
  keywords = {accurate heading estimation,accurate pedestrian indoor tracking,Cameras,Estimation,fast loop closure detection method,Gyroscopes,hybrid orientation filter,indoor radio,information filter,Infrastructureless,inherent sensor drift,iPDR system,long-term tracking,loop closure,mobile platforms,pedestrian dead reckoning,pedestrians,rapid loop detection,real-time indoor tracking,real-time system,Real-time systems,sensor data,sensor fusion,Sensors,smart phones,smartphone,target tracking,Target tracking,tracking trajectory,visual gyroscope,Visualization},
  annotation = {ZSCC: 0000001},
  file = {/Users/zyz/Zotero/storage/JEQXYRML/Yang et al. - 2019 - Real-Time Infrastructureless Indoor Tracking for P.pdf}
}

@article{Yang2019a,
  title = {Gridding Place Recognition for Fast Loop Closure Detection on Mobile Platforms},
  author = {Yang, Zhe and Pan, Yun and Huan, Ruohong and Bao, Yaoqi},
  year = {2019},
  journal = {Electronics Letters},
  volume = {55},
  number = {17},
  pages = {931--933},
  issn = {1350-911X},
  abstract = {Loop closure detection is an indispensable component to reduce the error accumulation in localisation applications, however, it is hard to be implemented as a real-time application with high accuracy on the mobile platform. A place recognition method called gridding place recognition (GPR) is proposed for ultra-fast loop closure detection with high accuracy maintained. The GPR splits the images into multiple grids for high parallelism and uses bags of word features to summarise each grid. The matching results for each grid are then retrieved and merged for further determination of loop closure under extra temporal and spatial constraints. The authors implement the method on a mobile platform, and the best accuracy and fastest execution time of 47 frames per second are achieved on several public benchmarks compared with other typical or fast loop closure algorithms.},
  copyright = {\textcopyright{} 2020 The Institution of Engineering and Technology},
  langid = {english},
  keywords = {error accumulation,fast loop closure algorithms,fastest execution time,GPR,high parallelism,image matching,indispensable component,localisation applications,mobile platform,mobile robots,multiple grids,place recognition method,real-time application,robot vision,SLAM (robots),typical loop closure algorithms,ultra-fast loop closure detection},
  annotation = {ZSCC: 0000003},
  file = {/Users/zyz/Zotero/storage/27W43ELZ/Yang et al. - 2019 - Gridding place recognition for fast loop closure d.pdf;/Users/zyz/Zotero/storage/MSBVJZNI/el.2019.html}
}

@phdthesis{Yang2019c,
  type = {博士},
  title = {无基础设施辅助的移动平台室内定位方法研究},
  author = {杨, 哲},
  year = {2019},
  copyright = {All rights reserved},
  school = {浙江大学},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/Users/zyz/Zotero/storage/V3CBW2AK/无基础设施辅助的移动平台室内定位方法研究_杨哲.caj}
}

@article{Yang2021,
  title = {{{PLSAV}}: {{Parallel}} Loop Searching and Verifying for Loop Closure Detection},
  shorttitle = {{{PLSAV}}},
  author = {Yang, Zhe and Pan, Yun and Deng, Lei and Xie, Yuan and Huan, Ruohong},
  year = {2021},
  journal = {IET Intelligent Transport Systems},
  volume = {15},
  number = {5},
  pages = {683--698},
  issn = {1751-9578},
  abstract = {Visual simultaneous localization and mapping (vSLAM), one of the most important applications in autonomous vehicles and robots to estimate the position and pose using inexpensive visual sensors, suffers from error accumulation for long-term navigation without loop closure detection. Recently, deep neural networks (DNNs) are leveraged to achieve high accuracy for loop closure detection, however the execution time is much slower than those employing handcrafted visual features. In this paper, a parallel loop searching and verifying method for loop closure detection with both high accuracy and high speed, which combines two parallel tasks using handcrafted and DNN features, respectively, is proposed. A fast loop searching is proposed to link the bag-of-words features and histogram for higher accuracy, and it splits the images into multiple grids for high parallelism; meanwhile, a DNN feature extractor is utilized for further verification. A loop state control method based on a finite state machine to control these tasks is designed, wherein the loop closure detection is described as a context-related procedure. The framework is implemented on a real machine, and the top-2 best accuracy and fastest execution time of 80-543 frames per second (min: 1.84ms, and max: 12.45ms) are achieved on several public benchmarks compared with some existing algorithms.},
  copyright = {\textcopyright{} 2021 The Authors. IET Intelligent Transport Systems published by John Wiley \& Sons Ltd on behalf of The Institution of Engineering and Technology},
  langid = {english},
  annotation = {ZSCC: 0000000},
  file = {/Users/zyz/Zotero/storage/FF4CPPY9/Yang et al. - 2021 - PLSAV Parallel loop searching and verifying for l.pdf;/Users/zyz/Zotero/storage/WY8R5S8L/itr2.html}
}

@article{Yang2022,
  title = {Comparing {{Cross-Subject Performance}} on {{Human Activities Recognition Using Learning Models}}},
  author = {Yang, Zhe and Qu, Mengjie and Pan, Yun and Huan, Ruohong},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {95179--95196},
  issn = {2169-3536},
  abstract = {Human activities recognition (HAR) plays a vital role in fields like ambient assisted living and health monitoring, in which cross-subject recognition is one of the main challenges coming from the diversity of various users. Although recent studies have achieved satisfactory results in a non-cross-subject condition, the recognition performance has significant degradation under the cross-subject criterion. In this paper, we evaluate three traditional machine learning methods and five deep neural network architectures under the same metrics on three popular HAR datasets: mHealth, PAMAP2, and UCIDSADS. The experimental results show that traditional machine learning approaches are generally more robust to the new subject scenarios under strict leave-one-subject-out cross-validation. Extra analysis indicates that hand-crafted features are one major reason for the better performance of traditional machine learning on cross-subject HAR, while deep learning is more prone to learning subject-dependent features under an end-to-end training process. A novel training strategy for decision-tree-based methods is also proposed in this paper, resulting in an improvement on the random forest model which achieves competitive performance at an average F1-score (accuracy) of 94.49\% (95.09\%), 91.64\% (92.21\%), and 92.70\% (93.29\%) on the three datasets, compared with state-of-the-art solutions for cross-subject HAR.},
  copyright = {All rights reserved},
  keywords = {Cross-subject,deep learning,Deep learning,Feature extraction,human activity recognition,Human factors,leave one subject out,Machine learning,Random forests,Sensors,Testing,traditional machine learning,Training data},
  file = {/Users/zyz/Zotero/storage/BEEWQ5US/Yang et al. - 2022 - Comparing Cross-Subject Performance on Human Activ.pdf;/Users/zyz/Zotero/storage/JSQ9WK8K/9878329.html}
}


